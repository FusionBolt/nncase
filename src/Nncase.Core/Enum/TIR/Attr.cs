namespace Nncase.TIR
{
    public enum Attr
    {

        /// <summary>
        /// Mark launching extent of thread, used by device API.
        /// </summary>
        thread_extent,
        /// <summary>
        /// Mark launching of a virtual thread.
        /// </summary>
        virtual_thread,
        /// <summary>
        /// Mark region is processed by a co-proccesor
        /// </summary>
        coproc_scope,
        /// <summary>
        /// Mark region creates coprocessor micro ops,
        ///  can be reused if corresponding variable is independent.
        /// </summary>
        coproc_uop_scope,
        /// <summary>
        /// Mark the scope as volatile access for certain handle. 
        /// </summary>
        volatile_scope,
        /// <summary>
        /// Mark the scope as generated by extern primitive.
        ///  such scope can contain arbitrary ir program and we need to be careful
        ///  when make certain assumptions about the structure of the program.
        /// </summary>
        extern_scope,
        /// <summary>
        /// Mark the scope as when computation start to happen
        ///  This can hint some code generator to create a new function for compute.
        /// </summary>
        compute_scope,
        /// <summary>
        /// Mark storage alignement requirement of buffers
        /// </summary>
        storage_alignment,
        /// <summary>
        /// Mark storage scope of realization
        /// </summary>
        realize_scope,
        /// <summary>
        /// The allocation device for global malloc in host.
        /// </summary>
        device_id,
        /// <summary>
        /// The device type.
        /// </summary>
        device_type,
        /// <summary>
        /// Mark of loop scope
        /// </summary>
        loop_scope,
        /// <summary>
        /// Mark of reduce scope
        /// </summary>
        reduce_scope,
        /// <summary>
        /// Mark region is guarded by the pragma extension
        /// </summary>
        pragma_scope_prefix,
        /// <summary>
        /// Import C source or file into the final code gen module
        /// </summary>
        pragma_import_c,
        /// <summary>
        /// Import llvm source or file into the final code gen module
        /// </summary>
        pragma_import_llvm,
        /// <summary>
        /// Try to modify the AST to support Tensor Core
        /// </summary>
        pragma_tensor_core,
        /// <summary>
        /// Mark of prefetch scope, value=offset,
        ///  run prefetch of Tensor on the current loop scope
        /// </summary>
        prefetch_scope,
        /// <summary>
        /// Marks production of double buffer data
        /// </summary>
        double_buffer_scope,
        /// <summary>
        /// Marks region used by double buffer write
        /// </summary>
        double_buffer_write,
        /// <summary>
        /// Mark of scan update scope 
        /// </summary>
        scan_update_scope,
        /// <summary>
        /// Mark of scan init scope 
        /// </summary>
        scan_init_scope,
        /// <summary>
        /// Mark alignment of buffer dimension
        ///  stmt.node is Tensor
        ///  stmt.value is tvm_tuple(dim, align, offset)
        ///  This gives hint to require stride of dim to be k * align + offset.
        /// </summary>
        buffer_dim_align,
        /// <summary>
        /// Mark stores/loads with theirs bounds.
        /// </summary>
        buffer_bound,
        /// <summary>
        /// Bind the buffer specification to the region of the op
        ///  When this scope occurs, the stmt.node is a IRArray<NodeRef> = [buffer, tensor]
        ///  stmt.value is a tvm_tuple(min0, extent0, min1, extent1, ...).
        ///  The scope represents that we need to bind the storage region of tensor to buffer.
        ///  This will affect replacement of some variables inside the scope that
        ///  corresponds to field of buffer to be the actual expressions of tensor during
        ///  storage flattening phase.
        /// </summary>
        buffer_bind_scope,
        /// <summary>
        /// channel read scope
        /// </summary>
        channel_read_scope,
        /// <summary>
        /// Advance step of channel after end of scope
        /// </summary>
        channel_read_advance,
        /// <summary>
        /// channel write scope
        /// </summary>
        channel_write_scope,
        /// <summary>
        /// Advance step of channel after end of scope
        /// </summary>
        channel_write_advance,
        /// <summary>
        /// pipeline stage scope, implies always execution
        /// </summary>
        pipeline_stage_scope,
        /// <summary>
        /// pipeline execution scope, implies the scope can be pipelined.
        /// </summary>
        pipeline_exec_scope,
        /// <summary>
        /// Mark that it is in the device scope.
        /// </summary>
        device_scope,
        /// <summary>
        /// Mark that the shape of TensorCore fragment
        /// </summary>
        fragment_shape,
        /// <summary>
        /// Mark that the layout of TensorCore fragment
        /// </summary>
        fragment_layout,
        /// <summary>
        /// Mark that the kernel is hand threaded and doesn't need syncs inserted
        /// </summary>
        hand_threaded,
        /// <summary>
        /// Mark whether the script-completer need to fill in missing access region
        /// during script parsing.
        /// <remarks>
        /// The result should be a integer mask with range [0, 4).
        /// if (mask & 1) the read region should be detected,
        /// if (mask & 2) the write region should be detected.
        /// </remarks>       
        /// </summary>
        script_parsing_detect_access,
        /// <summary>
        /// Mark that the loop should be partitioned.
        /// </summary>
        pragma_loop_partition_hint,

        /// <summary>
        /// List of thread IterVar that a DeviceLaunch function corresponds to.
        ///
        /// Type: Array<tir::IterVar>
        ///
        /// We call a device kernel launch function f using the following convention:
        ///
        /// Call(f,
        ///      [arg1, arg2, ..., arg_n,
        ///       work_size_1, work_size_2, ... work_size_m, dyn_shmem_size])
        ///
        /// Here n = len(arg), m = len(work_size) = len(device_thread_axis).
        ///
        /// When kDeviceUseDynSharedMemory is not set, dyn_shmem_size argument is omitted.
        ///
        /// The list of device_thread_axis indicates how can be bind the
        /// work_size arguments to the corresponding threads.
        /// </summary>
        DeviceThreadAxis,
        /// <summary>
        /// Whether or not use dynamic shared memory. 
        /// </summary>
        DeviceUseDynSharedMemory,
        /// <summary>
        /// Whether to set noalias rule on the function arguments.
        /// </summary>
        NoAlias,
        /// <summary>
        /// Mark the function as the entry function of the final generated runtime module.
        /// </summary>
        IsEntryFunc,
        /// <summary>
        /// Parameters used in the module that should be linked by the codegen.
        /// </summary>
        LinkedParams,
        /// <summary>
        /// Mark the function as the global function called from the host.
        /// </summary>
        IsGlobalFunc,
    }
}